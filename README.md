Project Overview

This project simulates an end-to-end data pipeline for Avalanche, a fictional Winter Sports Gear Company.
The pipeline integrates AWS S3, Snowflake, Snowpark, and Python (Pandas) to analyze order history and shipping logs.

âš™ï¸ Tech Stack

â˜ï¸ AWS S3 â€“ Raw data storage

â„ï¸ Snowflake â€“ Cloud Data Warehouse

ğŸ Snowpark (Python API) â€“ Scalable transformations

ğŸ“Š Pandas / Python â€“ Cleaning & validation

ğŸ“’ Jupyter Notebook â€“ Development & testing

ğŸ”„ Workflow

1ï¸âƒ£ Data Ingestion â€“ CSVs uploaded to AWS S3, connected to Snowflake external stage
2ï¸âƒ£ Data Cleaning & Transformation â€“ Pandas + Snowpark for joins, delay calculation, fact/dim tables
3ï¸âƒ£ Data Loading â€“ Transformed data stored in Snowflake tables
4ï¸âƒ£ Analytics â€“ KPIs like average delivery time, top products, and on-time shipment %

ğŸ“Š Example Insights

ğŸšš Average delivery time by carrier

â±ï¸ On-time vs delayed shipment %

ğŸ›’ Top-selling products & categories

ğŸŒ Regional sales distribution

ğŸ“˜ Key Learnings

âœ”ï¸ Hands-on with Snowpark DataFrames for scalable ETL
âœ”ï¸ Built a real-world style pipeline integrating AWS + Snowflake + Python
âœ”ï¸ Applied data modeling best practices (fact & dimension tables)

ğŸš€ Future Enhancements

Orchestrate pipeline with Airflow / Step Functions

Add real-time streaming with Kafka/MSK

Visualize results in Power BI / Tableau
